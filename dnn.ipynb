{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (None, 300)               1500      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 130)               39130     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 300)               39300     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 300)               90300     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 301       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 170,531\n",
      "Trainable params: 170,531\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential, optimizers                             \n",
    "from tensorflow.keras.layers import Dense                                       \n",
    "from tensorflow.keras.utils import normalize                                    \n",
    "from tensorflow.keras.callbacks import EarlyStopping                            \n",
    "import tensorflow as tf         \n",
    "activation = 'relu'\n",
    "optimizer='adam'  \n",
    "learning_rate=0.001 \n",
    "unit_1 = 300\n",
    "unit_2 = 130\n",
    "unit_3 = 300\n",
    "\n",
    "\n",
    "# hyperparameters not to be optimized\n",
    "l2_lambda = 0.001\n",
    "patience=200\n",
    "\n",
    "epochs=500\n",
    "batch_size=16 # 2^N form is preferred.\n",
    "kernel_regularizer=None\n",
    "\n",
    "model = Sequential()                                                          \n",
    "# determine the number of input features                                      \n",
    "n_features = train_scaled.shape[1]\n",
    "# 1st layer\n",
    "model.add(Dense(units=unit_1, activation=activation, kernel_initializer='he_normal',\n",
    "                kernel_regularizer=kernel_regularizer,                       \n",
    "                input_shape=(n_features,)))                                  \n",
    "# 2nd layer                                                                 \n",
    "model.add(Dense(units=unit_2, activation=activation, kernel_initializer='he_normal',\n",
    "                    kernel_regularizer=kernel_regularizer))                 \n",
    "# 3rd layer                                                                      \n",
    "model.add(Dense(units=unit_3, activation=activation, kernel_initializer='he_normal',\n",
    "                    kernel_regularizer=kernel_regularizer))        \n",
    "model.add(Dense(units=unit_3, activation=activation, kernel_initializer='he_normal',\n",
    "                    kernel_regularizer=kernel_regularizer))                 \n",
    "# linear activation                                          \n",
    "model.add(Dense(1))    \n",
    "\n",
    "model.summary()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "data  = df.to_numpy()\n",
    "X = data[:,:4]\n",
    "Y = data[:,4]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20)       \n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(X_train)\n",
    "test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor, BayesianRidge\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, plot_confusion_matrix, accuracy_score\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold, GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from numpy import absolute,std, mean\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 106.0299 - mae: 7.7162 - val_loss: 83.2984 - val_mae: 7.0740\n",
      "Epoch 2/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 76.6684 - mae: 6.4536 - val_loss: 66.9622 - val_mae: 5.8742\n",
      "Epoch 3/500\n",
      "90/90 [==============================] - 0s 907us/step - loss: 64.3049 - mae: 5.9025 - val_loss: 69.0714 - val_mae: 5.8524\n",
      "Epoch 4/500\n",
      "90/90 [==============================] - 0s 918us/step - loss: 56.5541 - mae: 5.4565 - val_loss: 65.3620 - val_mae: 5.7043\n",
      "Epoch 5/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 54.2759 - mae: 5.3057 - val_loss: 65.6778 - val_mae: 5.5942\n",
      "Epoch 6/500\n",
      "90/90 [==============================] - 0s 907us/step - loss: 51.3434 - mae: 5.1283 - val_loss: 63.1705 - val_mae: 5.4985\n",
      "Epoch 7/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 50.3486 - mae: 5.0240 - val_loss: 60.7318 - val_mae: 5.4317\n",
      "Epoch 8/500\n",
      "90/90 [==============================] - 0s 918us/step - loss: 49.9763 - mae: 5.0148 - val_loss: 54.6862 - val_mae: 4.9947\n",
      "Epoch 9/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 45.9490 - mae: 4.8814 - val_loss: 55.7644 - val_mae: 5.1525\n",
      "Epoch 10/500\n",
      "90/90 [==============================] - 0s 918us/step - loss: 42.9357 - mae: 4.6580 - val_loss: 52.8334 - val_mae: 4.9286\n",
      "Epoch 11/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 43.0187 - mae: 4.6667 - val_loss: 53.8292 - val_mae: 5.0719\n",
      "Epoch 12/500\n",
      "90/90 [==============================] - 0s 951us/step - loss: 40.4698 - mae: 4.4462 - val_loss: 50.6189 - val_mae: 4.8243\n",
      "Epoch 13/500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 38.3566 - mae: 4.3808 - val_loss: 51.5825 - val_mae: 4.8902\n",
      "Epoch 14/500\n",
      "90/90 [==============================] - 0s 997us/step - loss: 40.0628 - mae: 4.4168 - val_loss: 55.5752 - val_mae: 4.8333\n",
      "Epoch 15/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 37.9860 - mae: 4.2613 - val_loss: 47.5986 - val_mae: 4.8622\n",
      "Epoch 16/500\n",
      "90/90 [==============================] - 0s 907us/step - loss: 37.4090 - mae: 4.2094 - val_loss: 48.2663 - val_mae: 4.7062\n",
      "Epoch 17/500\n",
      "90/90 [==============================] - 0s 907us/step - loss: 36.5684 - mae: 4.2236 - val_loss: 46.5255 - val_mae: 4.4677\n",
      "Epoch 18/500\n",
      "90/90 [==============================] - 0s 997us/step - loss: 34.4624 - mae: 4.0188 - val_loss: 49.4404 - val_mae: 4.7687\n",
      "Epoch 19/500\n",
      "90/90 [==============================] - 0s 929us/step - loss: 34.5688 - mae: 4.0262 - val_loss: 46.7256 - val_mae: 4.4165\n",
      "Epoch 20/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 34.1738 - mae: 3.9825 - val_loss: 46.4758 - val_mae: 4.5591\n",
      "Epoch 21/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 33.4064 - mae: 3.9178 - val_loss: 47.5769 - val_mae: 4.6424\n",
      "Epoch 22/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 32.7758 - mae: 3.9015 - val_loss: 43.3038 - val_mae: 4.3074\n",
      "Epoch 23/500\n",
      "90/90 [==============================] - 0s 907us/step - loss: 34.6449 - mae: 4.0224 - val_loss: 44.9159 - val_mae: 4.5187\n",
      "Epoch 24/500\n",
      "90/90 [==============================] - 0s 898us/step - loss: 34.1173 - mae: 3.9356 - val_loss: 46.8358 - val_mae: 4.5123\n",
      "Epoch 25/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 32.5237 - mae: 3.8249 - val_loss: 45.7908 - val_mae: 4.4844\n",
      "Epoch 26/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 30.8903 - mae: 3.7917 - val_loss: 44.2524 - val_mae: 4.4202\n",
      "Epoch 27/500\n",
      "90/90 [==============================] - 0s 851us/step - loss: 32.1266 - mae: 3.8824 - val_loss: 42.9631 - val_mae: 4.3592\n",
      "Epoch 28/500\n",
      "90/90 [==============================] - 0s 907us/step - loss: 32.1311 - mae: 3.8144 - val_loss: 45.2909 - val_mae: 4.4909\n",
      "Epoch 29/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 30.1474 - mae: 3.6569 - val_loss: 45.5448 - val_mae: 4.2362\n",
      "Epoch 30/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 31.5895 - mae: 3.7448 - val_loss: 49.5103 - val_mae: 4.5958\n",
      "Epoch 31/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 31.7767 - mae: 3.7850 - val_loss: 44.6257 - val_mae: 4.3456\n",
      "Epoch 32/500\n",
      "90/90 [==============================] - 0s 941us/step - loss: 28.6596 - mae: 3.5573 - val_loss: 45.4530 - val_mae: 4.3204\n",
      "Epoch 33/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 33.4415 - mae: 3.9779 - val_loss: 43.5332 - val_mae: 4.2882\n",
      "Epoch 34/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 29.2626 - mae: 3.6258 - val_loss: 46.8658 - val_mae: 4.5430\n",
      "Epoch 35/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 29.6085 - mae: 3.6474 - val_loss: 43.4947 - val_mae: 4.3725\n",
      "Epoch 36/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 28.5856 - mae: 3.5705 - val_loss: 52.8182 - val_mae: 5.0990\n",
      "Epoch 37/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 30.2062 - mae: 3.7258 - val_loss: 45.1067 - val_mae: 4.4935\n",
      "Epoch 38/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 29.7538 - mae: 3.6846 - val_loss: 44.6933 - val_mae: 4.3812\n",
      "Epoch 39/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 27.4479 - mae: 3.4633 - val_loss: 43.4293 - val_mae: 4.2706\n",
      "Epoch 40/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 27.4894 - mae: 3.4971 - val_loss: 43.9781 - val_mae: 4.2092\n",
      "Epoch 41/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 26.9595 - mae: 3.4864 - val_loss: 42.6453 - val_mae: 4.2477\n",
      "Epoch 42/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 26.6091 - mae: 3.4193 - val_loss: 44.4917 - val_mae: 4.3293\n",
      "Epoch 43/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 27.0583 - mae: 3.4151 - val_loss: 43.4570 - val_mae: 4.2007\n",
      "Epoch 44/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 25.4233 - mae: 3.3510 - val_loss: 43.3771 - val_mae: 4.1436\n",
      "Epoch 45/500\n",
      "90/90 [==============================] - 0s 865us/step - loss: 26.9597 - mae: 3.3797 - val_loss: 50.0507 - val_mae: 4.7046\n",
      "Epoch 46/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 27.7142 - mae: 3.4541 - val_loss: 43.0512 - val_mae: 4.1121\n",
      "Epoch 47/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 26.3247 - mae: 3.3742 - val_loss: 44.3365 - val_mae: 4.4155\n",
      "Epoch 48/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 26.1299 - mae: 3.3114 - val_loss: 42.9394 - val_mae: 4.2024\n",
      "Epoch 49/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 25.4058 - mae: 3.3441 - val_loss: 43.9285 - val_mae: 4.1956\n",
      "Epoch 50/500\n",
      "90/90 [==============================] - 0s 907us/step - loss: 26.5316 - mae: 3.3276 - val_loss: 47.2267 - val_mae: 4.2858\n",
      "Epoch 51/500\n",
      "90/90 [==============================] - 0s 907us/step - loss: 26.9145 - mae: 3.3996 - val_loss: 40.6617 - val_mae: 4.1051\n",
      "Epoch 52/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 24.0032 - mae: 3.2808 - val_loss: 42.7111 - val_mae: 4.1699\n",
      "Epoch 53/500\n",
      "90/90 [==============================] - 0s 963us/step - loss: 24.8995 - mae: 3.3029 - val_loss: 43.9787 - val_mae: 4.1024\n",
      "Epoch 54/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 24.9768 - mae: 3.2854 - val_loss: 41.0856 - val_mae: 4.1800\n",
      "Epoch 55/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 27.1185 - mae: 3.3760 - val_loss: 43.3398 - val_mae: 4.2005\n",
      "Epoch 56/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 24.6986 - mae: 3.2788 - val_loss: 43.9989 - val_mae: 4.1544\n",
      "Epoch 57/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 24.5551 - mae: 3.2200 - val_loss: 40.0510 - val_mae: 4.0318\n",
      "Epoch 58/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 23.4608 - mae: 3.1333 - val_loss: 41.5398 - val_mae: 4.0066\n",
      "Epoch 59/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 24.6185 - mae: 3.2895 - val_loss: 44.6648 - val_mae: 4.1465\n",
      "Epoch 60/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 23.9582 - mae: 3.2007 - val_loss: 41.1003 - val_mae: 4.2146\n",
      "Epoch 61/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 23.5966 - mae: 3.2002 - val_loss: 42.2011 - val_mae: 4.1633\n",
      "Epoch 62/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 22.9742 - mae: 3.1369 - val_loss: 40.7448 - val_mae: 4.0918\n",
      "Epoch 63/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 23.2780 - mae: 3.1851 - val_loss: 40.4525 - val_mae: 4.0977\n",
      "Epoch 64/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 23.1561 - mae: 3.1882 - val_loss: 41.6186 - val_mae: 3.9728\n",
      "Epoch 65/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 23.3146 - mae: 3.1600 - val_loss: 43.8971 - val_mae: 4.3048\n",
      "Epoch 66/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 23.8591 - mae: 3.2467 - val_loss: 39.9106 - val_mae: 3.9565\n",
      "Epoch 67/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 24.7578 - mae: 3.2214 - val_loss: 41.2296 - val_mae: 4.1044\n",
      "Epoch 68/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 23.6586 - mae: 3.1828 - val_loss: 42.9492 - val_mae: 4.0117\n",
      "Epoch 69/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 22.7378 - mae: 3.0815 - val_loss: 44.0577 - val_mae: 4.0794\n",
      "Epoch 70/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 22.6777 - mae: 3.1325 - val_loss: 41.4196 - val_mae: 3.9467\n",
      "Epoch 71/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 23.0633 - mae: 3.1184 - val_loss: 41.6030 - val_mae: 4.0610\n",
      "Epoch 72/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 24.4698 - mae: 3.2383 - val_loss: 42.1377 - val_mae: 3.9776\n",
      "Epoch 73/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 22.8601 - mae: 3.1329 - val_loss: 40.5036 - val_mae: 3.8461\n",
      "Epoch 74/500\n",
      "90/90 [==============================] - 0s 963us/step - loss: 23.0808 - mae: 3.1321 - val_loss: 43.4880 - val_mae: 4.1095\n",
      "Epoch 75/500\n",
      "90/90 [==============================] - 0s 851us/step - loss: 22.8306 - mae: 3.1155 - val_loss: 41.2500 - val_mae: 3.9332\n",
      "Epoch 76/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 21.9357 - mae: 3.0372 - val_loss: 42.2454 - val_mae: 4.0043\n",
      "Epoch 77/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 21.6065 - mae: 3.0134 - val_loss: 39.6595 - val_mae: 3.8355\n",
      "Epoch 78/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 21.3150 - mae: 2.9476 - val_loss: 43.6056 - val_mae: 4.3038\n",
      "Epoch 79/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 23.8728 - mae: 3.1847 - val_loss: 44.4728 - val_mae: 3.9347\n",
      "Epoch 80/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 21.2274 - mae: 3.0020 - val_loss: 41.5623 - val_mae: 3.9754\n",
      "Epoch 81/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 22.1585 - mae: 3.0675 - val_loss: 40.9683 - val_mae: 3.8469\n",
      "Epoch 82/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 22.1486 - mae: 3.0298 - val_loss: 44.1970 - val_mae: 4.1779\n",
      "Epoch 83/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 21.1411 - mae: 2.9734 - val_loss: 41.0122 - val_mae: 4.0781\n",
      "Epoch 84/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 21.4654 - mae: 2.9839 - val_loss: 41.9535 - val_mae: 4.0338\n",
      "Epoch 85/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 21.6866 - mae: 3.0294 - val_loss: 42.3918 - val_mae: 3.8725\n",
      "Epoch 86/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 21.5055 - mae: 2.9630 - val_loss: 43.0535 - val_mae: 4.1093\n",
      "Epoch 87/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 20.8429 - mae: 2.9788 - val_loss: 42.3572 - val_mae: 3.9105\n",
      "Epoch 88/500\n",
      "90/90 [==============================] - 0s 918us/step - loss: 21.6882 - mae: 2.9918 - val_loss: 40.4779 - val_mae: 4.0167\n",
      "Epoch 89/500\n",
      "90/90 [==============================] - 0s 851us/step - loss: 22.5156 - mae: 3.1226 - val_loss: 42.9120 - val_mae: 3.7884\n",
      "Epoch 90/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 20.7957 - mae: 2.9566 - val_loss: 43.3699 - val_mae: 4.1023\n",
      "Epoch 91/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 21.2681 - mae: 3.0133 - val_loss: 44.1730 - val_mae: 3.9467\n",
      "Epoch 92/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 20.7843 - mae: 2.8859 - val_loss: 41.9437 - val_mae: 3.7539\n",
      "Epoch 93/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 21.0718 - mae: 2.9463 - val_loss: 43.3691 - val_mae: 3.9396\n",
      "Epoch 94/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 20.6232 - mae: 2.9327 - val_loss: 42.6685 - val_mae: 3.9056\n",
      "Epoch 95/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 20.6447 - mae: 2.9104 - val_loss: 43.4856 - val_mae: 3.9358\n",
      "Epoch 96/500\n",
      "90/90 [==============================] - 0s 872us/step - loss: 21.0027 - mae: 2.9388 - val_loss: 41.5522 - val_mae: 3.8519\n",
      "Epoch 97/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 21.8448 - mae: 3.0366 - val_loss: 45.3379 - val_mae: 4.1411\n",
      "Epoch 98/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 21.6608 - mae: 3.0161 - val_loss: 43.1945 - val_mae: 3.9191\n",
      "Epoch 99/500\n",
      "90/90 [==============================] - 0s 952us/step - loss: 21.1115 - mae: 2.9919 - val_loss: 42.8094 - val_mae: 3.8924\n",
      "Epoch 100/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 21.1626 - mae: 2.9307 - val_loss: 44.1543 - val_mae: 3.9598\n",
      "Epoch 101/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 20.9937 - mae: 2.9780 - val_loss: 43.6603 - val_mae: 4.0384\n",
      "Epoch 102/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 20.7517 - mae: 2.9548 - val_loss: 43.0036 - val_mae: 3.8494\n",
      "Epoch 103/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 21.1352 - mae: 2.9734 - val_loss: 43.4248 - val_mae: 4.1856\n",
      "Epoch 104/500\n",
      "90/90 [==============================] - 0s 907us/step - loss: 21.1710 - mae: 2.9321 - val_loss: 43.4954 - val_mae: 3.8295\n",
      "Epoch 105/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 20.9961 - mae: 2.9233 - val_loss: 42.5325 - val_mae: 3.8800\n",
      "Epoch 106/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 20.6176 - mae: 2.9212 - val_loss: 43.9297 - val_mae: 3.9443\n",
      "Epoch 107/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 20.4876 - mae: 2.8825 - val_loss: 44.7752 - val_mae: 3.9139\n",
      "Epoch 108/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 20.9405 - mae: 2.9418 - val_loss: 44.9559 - val_mae: 3.9218\n",
      "Epoch 109/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 20.6945 - mae: 2.9568 - val_loss: 43.0131 - val_mae: 4.0372\n",
      "Epoch 110/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 20.7537 - mae: 2.9328 - val_loss: 45.7532 - val_mae: 3.9659\n",
      "Epoch 111/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 20.2754 - mae: 2.8511 - val_loss: 44.2875 - val_mae: 3.9070\n",
      "Epoch 112/500\n",
      "90/90 [==============================] - 0s 872us/step - loss: 20.5077 - mae: 2.9426 - val_loss: 44.8187 - val_mae: 3.9207\n",
      "Epoch 113/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 19.9477 - mae: 2.8341 - val_loss: 42.1535 - val_mae: 3.7862\n",
      "Epoch 114/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 20.2773 - mae: 2.8508 - val_loss: 45.2204 - val_mae: 4.0257\n",
      "Epoch 115/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 20.6921 - mae: 2.9357 - val_loss: 46.2270 - val_mae: 4.2219\n",
      "Epoch 116/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 20.2038 - mae: 2.8685 - val_loss: 42.2777 - val_mae: 3.9467\n",
      "Epoch 117/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 19.9356 - mae: 2.8886 - val_loss: 42.6094 - val_mae: 3.9268\n",
      "Epoch 118/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 19.9036 - mae: 2.8369 - val_loss: 41.6424 - val_mae: 3.7601\n",
      "Epoch 119/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 20.1359 - mae: 2.8570 - val_loss: 44.3591 - val_mae: 3.9692\n",
      "Epoch 120/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 21.1024 - mae: 2.8933 - val_loss: 43.3344 - val_mae: 3.9411\n",
      "Epoch 121/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 20.3582 - mae: 2.9569 - val_loss: 44.6992 - val_mae: 4.1220\n",
      "Epoch 122/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 20.7638 - mae: 2.9099 - val_loss: 43.7299 - val_mae: 4.0594\n",
      "Epoch 123/500\n",
      "90/90 [==============================] - 0s 918us/step - loss: 20.0905 - mae: 2.8549 - val_loss: 45.0793 - val_mae: 3.9678\n",
      "Epoch 124/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 19.8211 - mae: 2.7822 - val_loss: 44.6005 - val_mae: 4.1716\n",
      "Epoch 125/500\n",
      "90/90 [==============================] - 0s 963us/step - loss: 21.0177 - mae: 3.0013 - val_loss: 43.8000 - val_mae: 3.9030\n",
      "Epoch 126/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 20.1745 - mae: 2.8543 - val_loss: 42.3644 - val_mae: 3.8563\n",
      "Epoch 127/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 19.7391 - mae: 2.7750 - val_loss: 44.9292 - val_mae: 3.9296\n",
      "Epoch 128/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 19.6297 - mae: 2.8011 - val_loss: 43.5306 - val_mae: 3.9818\n",
      "Epoch 129/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 19.5556 - mae: 2.7819 - val_loss: 43.0546 - val_mae: 3.8875\n",
      "Epoch 130/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 19.8787 - mae: 2.8268 - val_loss: 42.9895 - val_mae: 3.7771\n",
      "Epoch 131/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 19.4127 - mae: 2.7662 - val_loss: 43.0891 - val_mae: 3.8019\n",
      "Epoch 132/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 19.1014 - mae: 2.6966 - val_loss: 44.2369 - val_mae: 3.8880\n",
      "Epoch 133/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 19.3459 - mae: 2.7990 - val_loss: 46.3958 - val_mae: 4.4979\n",
      "Epoch 134/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 21.1418 - mae: 2.9665 - val_loss: 44.4637 - val_mae: 3.9897\n",
      "Epoch 135/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 20.4491 - mae: 2.8650 - val_loss: 42.9602 - val_mae: 3.9652\n",
      "Epoch 136/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 20.0381 - mae: 2.8234 - val_loss: 45.4026 - val_mae: 3.9941\n",
      "Epoch 137/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 19.8109 - mae: 2.8103 - val_loss: 44.6142 - val_mae: 4.0596\n",
      "Epoch 138/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 19.7176 - mae: 2.8035 - val_loss: 44.1987 - val_mae: 3.8727\n",
      "Epoch 139/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 20.3142 - mae: 2.8555 - val_loss: 45.8224 - val_mae: 4.2474\n",
      "Epoch 140/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 19.4180 - mae: 2.7972 - val_loss: 44.3398 - val_mae: 3.9739\n",
      "Epoch 141/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 19.2691 - mae: 2.7523 - val_loss: 45.6354 - val_mae: 3.9725\n",
      "Epoch 142/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 19.0494 - mae: 2.7370 - val_loss: 44.5051 - val_mae: 3.9284\n",
      "Epoch 143/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 19.0960 - mae: 2.7249 - val_loss: 43.1680 - val_mae: 3.8666\n",
      "Epoch 144/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 19.9879 - mae: 2.8117 - val_loss: 44.5606 - val_mae: 3.9542\n",
      "Epoch 145/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 19.8986 - mae: 2.7780 - val_loss: 45.4433 - val_mae: 3.8839\n",
      "Epoch 146/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 19.3907 - mae: 2.7674 - val_loss: 43.9008 - val_mae: 3.9209\n",
      "Epoch 147/500\n",
      "90/90 [==============================] - 0s 884us/step - loss: 20.0723 - mae: 2.8375 - val_loss: 44.4102 - val_mae: 3.9493\n",
      "Epoch 148/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 19.6006 - mae: 2.8130 - val_loss: 42.2629 - val_mae: 3.8545\n",
      "Epoch 149/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 19.8832 - mae: 2.8342 - val_loss: 43.2995 - val_mae: 3.8504\n",
      "Epoch 150/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 21.5001 - mae: 2.8748 - val_loss: 44.7753 - val_mae: 3.8906\n",
      "Epoch 151/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 19.3638 - mae: 2.8117 - val_loss: 44.1128 - val_mae: 4.0359\n",
      "Epoch 152/500\n",
      "90/90 [==============================] - 0s 952us/step - loss: 19.3438 - mae: 2.7988 - val_loss: 44.9697 - val_mae: 3.8848\n",
      "Epoch 153/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 19.4968 - mae: 2.7490 - val_loss: 44.9367 - val_mae: 3.9271\n",
      "Epoch 154/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 19.9604 - mae: 2.8148 - val_loss: 45.6579 - val_mae: 4.0244\n",
      "Epoch 155/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 20.0519 - mae: 2.8304 - val_loss: 45.3291 - val_mae: 3.9755\n",
      "Epoch 156/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 19.3969 - mae: 2.7537 - val_loss: 44.9336 - val_mae: 3.8394\n",
      "Epoch 157/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.7573 - mae: 2.6664 - val_loss: 43.5960 - val_mae: 3.8797\n",
      "Epoch 158/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 19.0408 - mae: 2.7681 - val_loss: 44.4020 - val_mae: 3.9191\n",
      "Epoch 159/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 19.4935 - mae: 2.7819 - val_loss: 44.0990 - val_mae: 3.8058\n",
      "Epoch 160/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 19.2649 - mae: 2.7568 - val_loss: 44.3783 - val_mae: 3.9250\n",
      "Epoch 161/500\n",
      "90/90 [==============================] - 0s 907us/step - loss: 18.8920 - mae: 2.7341 - val_loss: 43.1840 - val_mae: 3.8455\n",
      "Epoch 162/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 19.3351 - mae: 2.7643 - val_loss: 45.3476 - val_mae: 4.1343\n",
      "Epoch 163/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.7247 - mae: 2.7171 - val_loss: 44.1445 - val_mae: 3.8802\n",
      "Epoch 164/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 19.4621 - mae: 2.7552 - val_loss: 45.6032 - val_mae: 4.0522\n",
      "Epoch 165/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 19.5791 - mae: 2.8365 - val_loss: 46.5550 - val_mae: 4.1118\n",
      "Epoch 166/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 19.1870 - mae: 2.7335 - val_loss: 42.6126 - val_mae: 3.8583\n",
      "Epoch 167/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 19.4978 - mae: 2.7832 - val_loss: 45.8248 - val_mae: 4.1246\n",
      "Epoch 168/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 20.8185 - mae: 2.8349 - val_loss: 45.6462 - val_mae: 3.9867\n",
      "Epoch 169/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 20.8690 - mae: 2.9105 - val_loss: 48.0309 - val_mae: 4.4662\n",
      "Epoch 170/500\n",
      "90/90 [==============================] - 0s 918us/step - loss: 20.2393 - mae: 2.8934 - val_loss: 45.7538 - val_mae: 3.9726\n",
      "Epoch 171/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 19.4807 - mae: 2.7759 - val_loss: 43.1914 - val_mae: 3.9651\n",
      "Epoch 172/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 19.7600 - mae: 2.8638 - val_loss: 43.2266 - val_mae: 3.9590\n",
      "Epoch 173/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.9661 - mae: 2.7331 - val_loss: 44.3263 - val_mae: 3.7986\n",
      "Epoch 174/500\n",
      "90/90 [==============================] - 0s 907us/step - loss: 19.1231 - mae: 2.7480 - val_loss: 45.3088 - val_mae: 3.8184\n",
      "Epoch 175/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 19.2241 - mae: 2.7852 - val_loss: 45.0446 - val_mae: 3.8081\n",
      "Epoch 176/500\n",
      "90/90 [==============================] - 0s 907us/step - loss: 18.9541 - mae: 2.7246 - val_loss: 46.1884 - val_mae: 3.9351\n",
      "Epoch 177/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 18.8085 - mae: 2.6730 - val_loss: 44.9040 - val_mae: 4.0328\n",
      "Epoch 178/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 19.0160 - mae: 2.7220 - val_loss: 44.9645 - val_mae: 3.8027\n",
      "Epoch 179/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.6112 - mae: 2.6716 - val_loss: 44.4057 - val_mae: 3.9042\n",
      "Epoch 180/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 18.7811 - mae: 2.7256 - val_loss: 45.8494 - val_mae: 4.0988\n",
      "Epoch 181/500\n",
      "90/90 [==============================] - 0s 884us/step - loss: 18.5266 - mae: 2.6437 - val_loss: 44.9117 - val_mae: 3.8877\n",
      "Epoch 182/500\n",
      "90/90 [==============================] - 0s 952us/step - loss: 18.3343 - mae: 2.6429 - val_loss: 44.1999 - val_mae: 3.8994\n",
      "Epoch 183/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 19.1394 - mae: 2.7506 - val_loss: 45.0496 - val_mae: 3.8604\n",
      "Epoch 184/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 19.1144 - mae: 2.7471 - val_loss: 44.9226 - val_mae: 3.8821\n",
      "Epoch 185/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 19.3286 - mae: 2.7105 - val_loss: 44.8703 - val_mae: 3.7607\n",
      "Epoch 186/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 18.8104 - mae: 2.7194 - val_loss: 44.3469 - val_mae: 3.7512\n",
      "Epoch 187/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.9375 - mae: 2.7161 - val_loss: 46.5548 - val_mae: 4.1374\n",
      "Epoch 188/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 19.3323 - mae: 2.8147 - val_loss: 45.4470 - val_mae: 4.0124\n",
      "Epoch 189/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 19.5166 - mae: 2.7879 - val_loss: 44.3302 - val_mae: 3.9387\n",
      "Epoch 190/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.7129 - mae: 2.7112 - val_loss: 45.2578 - val_mae: 4.0885\n",
      "Epoch 191/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.6392 - mae: 2.6684 - val_loss: 42.6540 - val_mae: 3.8857\n",
      "Epoch 192/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 19.1096 - mae: 2.7162 - val_loss: 44.2161 - val_mae: 3.8111\n",
      "Epoch 193/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.6282 - mae: 2.6906 - val_loss: 44.5783 - val_mae: 3.9589\n",
      "Epoch 194/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.3361 - mae: 2.6558 - val_loss: 47.0840 - val_mae: 4.0045\n",
      "Epoch 195/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 19.1196 - mae: 2.7868 - val_loss: 44.1697 - val_mae: 3.8005\n",
      "Epoch 196/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.3187 - mae: 2.6385 - val_loss: 44.6024 - val_mae: 3.9003\n",
      "Epoch 197/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 19.1828 - mae: 2.7326 - val_loss: 44.4165 - val_mae: 3.8986\n",
      "Epoch 198/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 22.8436 - mae: 3.0085 - val_loss: 47.5966 - val_mae: 4.0337\n",
      "Epoch 199/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 23.2816 - mae: 3.0522 - val_loss: 43.9933 - val_mae: 4.0489\n",
      "Epoch 200/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 22.0423 - mae: 3.0466 - val_loss: 46.5252 - val_mae: 4.1031\n",
      "Epoch 201/500\n",
      "90/90 [==============================] - 0s 941us/step - loss: 19.8224 - mae: 2.7680 - val_loss: 44.5047 - val_mae: 3.8740\n",
      "Epoch 202/500\n",
      "90/90 [==============================] - 0s 963us/step - loss: 18.7304 - mae: 2.7148 - val_loss: 44.1077 - val_mae: 3.7938\n",
      "Epoch 203/500\n",
      "90/90 [==============================] - 0s 952us/step - loss: 18.7829 - mae: 2.6944 - val_loss: 45.0132 - val_mae: 3.9091\n",
      "Epoch 204/500\n",
      "90/90 [==============================] - 0s 918us/step - loss: 18.9563 - mae: 2.7468 - val_loss: 45.0995 - val_mae: 3.9247\n",
      "Epoch 205/500\n",
      "90/90 [==============================] - 0s 918us/step - loss: 18.3118 - mae: 2.6620 - val_loss: 45.0970 - val_mae: 3.8684\n",
      "Epoch 206/500\n",
      "90/90 [==============================] - 0s 929us/step - loss: 18.2691 - mae: 2.6238 - val_loss: 45.4722 - val_mae: 3.8693\n",
      "Epoch 207/500\n",
      "90/90 [==============================] - 0s 929us/step - loss: 18.5015 - mae: 2.6519 - val_loss: 44.8559 - val_mae: 3.7617\n",
      "Epoch 208/500\n",
      "90/90 [==============================] - 0s 907us/step - loss: 18.4785 - mae: 2.6678 - val_loss: 44.6449 - val_mae: 3.9120\n",
      "Epoch 209/500\n",
      "90/90 [==============================] - 0s 918us/step - loss: 18.5899 - mae: 2.6781 - val_loss: 43.8115 - val_mae: 3.8485\n",
      "Epoch 210/500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 18.3462 - mae: 2.6329 - val_loss: 46.6983 - val_mae: 3.9697\n",
      "Epoch 211/500\n",
      "90/90 [==============================] - 0s 941us/step - loss: 18.4659 - mae: 2.6583 - val_loss: 45.3403 - val_mae: 3.8582\n",
      "Epoch 212/500\n",
      "90/90 [==============================] - 0s 952us/step - loss: 18.5329 - mae: 2.6608 - val_loss: 47.1215 - val_mae: 4.2505\n",
      "Epoch 213/500\n",
      "90/90 [==============================] - 0s 929us/step - loss: 19.6129 - mae: 2.7994 - val_loss: 45.2389 - val_mae: 3.8716\n",
      "Epoch 214/500\n",
      "90/90 [==============================] - 0s 895us/step - loss: 18.4289 - mae: 2.6495 - val_loss: 44.1588 - val_mae: 3.8779\n",
      "Epoch 215/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 18.6218 - mae: 2.6783 - val_loss: 44.8506 - val_mae: 3.8738\n",
      "Epoch 216/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.5132 - mae: 2.6630 - val_loss: 44.9461 - val_mae: 3.8299\n",
      "Epoch 217/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.6021 - mae: 2.7140 - val_loss: 41.6503 - val_mae: 3.8093\n",
      "Epoch 218/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.4724 - mae: 2.7123 - val_loss: 44.8414 - val_mae: 3.8555\n",
      "Epoch 219/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.1251 - mae: 2.5950 - val_loss: 44.9537 - val_mae: 3.8740\n",
      "Epoch 220/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.7353 - mae: 2.6987 - val_loss: 45.8092 - val_mae: 3.9201\n",
      "Epoch 221/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 19.2404 - mae: 2.7777 - val_loss: 46.0292 - val_mae: 3.9150\n",
      "Epoch 222/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.5814 - mae: 2.6513 - val_loss: 44.8146 - val_mae: 3.8619\n",
      "Epoch 223/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.8746 - mae: 2.7506 - val_loss: 44.7966 - val_mae: 3.8849\n",
      "Epoch 224/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.5582 - mae: 2.6622 - val_loss: 44.9160 - val_mae: 3.8511\n",
      "Epoch 225/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.2498 - mae: 2.6214 - val_loss: 45.2171 - val_mae: 3.8543\n",
      "Epoch 226/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.2356 - mae: 2.6424 - val_loss: 45.6497 - val_mae: 3.8350\n",
      "Epoch 227/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 18.6296 - mae: 2.7047 - val_loss: 44.3209 - val_mae: 4.0476\n",
      "Epoch 228/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 20.0566 - mae: 2.8246 - val_loss: 44.6524 - val_mae: 3.9456\n",
      "Epoch 229/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 19.4023 - mae: 2.7312 - val_loss: 44.2994 - val_mae: 3.8608\n",
      "Epoch 230/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.7420 - mae: 2.6901 - val_loss: 44.4507 - val_mae: 3.9406\n",
      "Epoch 231/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.5955 - mae: 2.6626 - val_loss: 44.6813 - val_mae: 3.9020\n",
      "Epoch 232/500\n",
      "90/90 [==============================] - 0s 877us/step - loss: 18.4642 - mae: 2.6673 - val_loss: 45.9476 - val_mae: 3.9265\n",
      "Epoch 233/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.5902 - mae: 2.6761 - val_loss: 45.6468 - val_mae: 3.9270\n",
      "Epoch 234/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.3395 - mae: 2.6137 - val_loss: 44.0810 - val_mae: 3.8003\n",
      "Epoch 235/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.1727 - mae: 2.6117 - val_loss: 46.4070 - val_mae: 3.8446\n",
      "Epoch 236/500\n",
      "90/90 [==============================] - 0s 985us/step - loss: 18.3358 - mae: 2.6769 - val_loss: 44.1900 - val_mae: 3.9030\n",
      "Epoch 237/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.3453 - mae: 2.6363 - val_loss: 45.3085 - val_mae: 3.9325\n",
      "Epoch 238/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.3042 - mae: 2.6216 - val_loss: 45.0756 - val_mae: 3.7989\n",
      "Epoch 239/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.3395 - mae: 2.6846 - val_loss: 44.7447 - val_mae: 3.8657\n",
      "Epoch 240/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.8950 - mae: 2.7098 - val_loss: 45.9478 - val_mae: 3.8867\n",
      "Epoch 241/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.3761 - mae: 2.6666 - val_loss: 46.9916 - val_mae: 3.9479\n",
      "Epoch 242/500\n",
      "90/90 [==============================] - 0s 907us/step - loss: 18.7754 - mae: 2.6894 - val_loss: 46.0348 - val_mae: 3.9316\n",
      "Epoch 243/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 18.0605 - mae: 2.5855 - val_loss: 46.4724 - val_mae: 3.9518\n",
      "Epoch 244/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 18.8356 - mae: 2.7368 - val_loss: 46.0631 - val_mae: 4.0520\n",
      "Epoch 245/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.3789 - mae: 2.6306 - val_loss: 46.7386 - val_mae: 3.9596\n",
      "Epoch 246/500\n",
      "90/90 [==============================] - 0s 907us/step - loss: 18.1137 - mae: 2.6293 - val_loss: 45.4219 - val_mae: 3.8938\n",
      "Epoch 247/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.4574 - mae: 2.6934 - val_loss: 45.8941 - val_mae: 3.8920\n",
      "Epoch 248/500\n",
      "90/90 [==============================] - 0s 918us/step - loss: 18.3096 - mae: 2.6566 - val_loss: 44.8891 - val_mae: 3.9490\n",
      "Epoch 249/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 18.4064 - mae: 2.6412 - val_loss: 45.4890 - val_mae: 3.8977\n",
      "Epoch 250/500\n",
      "90/90 [==============================] - 0s 918us/step - loss: 18.2535 - mae: 2.6327 - val_loss: 47.8339 - val_mae: 3.9031\n",
      "Epoch 251/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 18.2813 - mae: 2.6333 - val_loss: 45.5129 - val_mae: 3.8637\n",
      "Epoch 252/500\n",
      "90/90 [==============================] - 0s 918us/step - loss: 18.3982 - mae: 2.6852 - val_loss: 45.9798 - val_mae: 4.0972\n",
      "Epoch 253/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 18.3673 - mae: 2.6569 - val_loss: 45.3614 - val_mae: 3.9071\n",
      "Epoch 254/500\n",
      "90/90 [==============================] - 0s 907us/step - loss: 19.1314 - mae: 2.7416 - val_loss: 46.7475 - val_mae: 3.8888\n",
      "Epoch 255/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 18.0624 - mae: 2.6353 - val_loss: 46.0615 - val_mae: 4.0057\n",
      "Epoch 256/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 18.3764 - mae: 2.6790 - val_loss: 46.1302 - val_mae: 4.0027\n",
      "Epoch 257/500\n",
      "90/90 [==============================] - 0s 918us/step - loss: 18.5065 - mae: 2.6453 - val_loss: 47.4120 - val_mae: 4.0064\n",
      "Epoch 258/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 18.4754 - mae: 2.6507 - val_loss: 46.0406 - val_mae: 3.9277\n",
      "Epoch 259/500\n",
      "90/90 [==============================] - 0s 918us/step - loss: 18.8180 - mae: 2.7302 - val_loss: 48.6133 - val_mae: 4.4581\n",
      "Epoch 260/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 18.6487 - mae: 2.7606 - val_loss: 48.2316 - val_mae: 4.1072\n",
      "Epoch 261/500\n",
      "90/90 [==============================] - 0s 941us/step - loss: 17.9696 - mae: 2.6335 - val_loss: 45.7996 - val_mae: 4.0170\n",
      "Epoch 262/500\n",
      "90/90 [==============================] - 0s 985us/step - loss: 18.7567 - mae: 2.6872 - val_loss: 47.3902 - val_mae: 3.8662\n",
      "Epoch 263/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.1339 - mae: 2.6232 - val_loss: 45.4692 - val_mae: 3.9174\n",
      "Epoch 264/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.8900 - mae: 2.6113 - val_loss: 46.9531 - val_mae: 4.0990\n",
      "Epoch 265/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 18.4371 - mae: 2.6402 - val_loss: 45.0447 - val_mae: 3.8781\n",
      "Epoch 266/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 18.2113 - mae: 2.6365 - val_loss: 46.4361 - val_mae: 3.8705\n",
      "Epoch 267/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 18.2334 - mae: 2.6377 - val_loss: 44.1624 - val_mae: 3.8972\n",
      "Epoch 268/500\n",
      "90/90 [==============================] - 0s 941us/step - loss: 18.5206 - mae: 2.6960 - val_loss: 46.2764 - val_mae: 3.8778\n",
      "Epoch 269/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.2698 - mae: 2.6443 - val_loss: 44.9111 - val_mae: 3.8463\n",
      "Epoch 270/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.5943 - mae: 2.6254 - val_loss: 45.3796 - val_mae: 3.7860\n",
      "Epoch 271/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 18.3563 - mae: 2.6371 - val_loss: 45.5795 - val_mae: 3.9113\n",
      "Epoch 272/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.3496 - mae: 2.6276 - val_loss: 45.1325 - val_mae: 4.0497\n",
      "Epoch 273/500\n",
      "90/90 [==============================] - 0s 907us/step - loss: 18.0124 - mae: 2.6332 - val_loss: 45.8775 - val_mae: 3.8422\n",
      "Epoch 274/500\n",
      "90/90 [==============================] - 0s 907us/step - loss: 18.0544 - mae: 2.6086 - val_loss: 45.0061 - val_mae: 3.9823\n",
      "Epoch 275/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.0726 - mae: 2.6317 - val_loss: 47.5936 - val_mae: 3.9039\n",
      "Epoch 276/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 18.1389 - mae: 2.6280 - val_loss: 45.7684 - val_mae: 3.8248\n",
      "Epoch 277/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.4534 - mae: 2.6401 - val_loss: 46.1037 - val_mae: 4.0836\n",
      "Epoch 278/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 18.1135 - mae: 2.5976 - val_loss: 45.2883 - val_mae: 3.8165\n",
      "Epoch 279/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.1714 - mae: 2.6371 - val_loss: 44.6369 - val_mae: 3.8034\n",
      "Epoch 280/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.9149 - mae: 2.5993 - val_loss: 45.4832 - val_mae: 3.9200\n",
      "Epoch 281/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 17.8428 - mae: 2.5906 - val_loss: 44.5409 - val_mae: 3.8287\n",
      "Epoch 282/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.1910 - mae: 2.7002 - val_loss: 45.1982 - val_mae: 3.8625\n",
      "Epoch 283/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.1606 - mae: 2.6385 - val_loss: 45.9322 - val_mae: 4.0215\n",
      "Epoch 284/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 18.1127 - mae: 2.5952 - val_loss: 46.2223 - val_mae: 4.0955\n",
      "Epoch 285/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 19.0625 - mae: 2.7016 - val_loss: 44.8010 - val_mae: 3.8141\n",
      "Epoch 286/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.9231 - mae: 2.5902 - val_loss: 45.7062 - val_mae: 3.8978\n",
      "Epoch 287/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.2428 - mae: 2.6471 - val_loss: 46.0985 - val_mae: 3.9613\n",
      "Epoch 288/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.6887 - mae: 2.6894 - val_loss: 45.9397 - val_mae: 3.9649\n",
      "Epoch 289/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.6022 - mae: 2.6759 - val_loss: 45.9937 - val_mae: 3.9767\n",
      "Epoch 290/500\n",
      "90/90 [==============================] - 0s 974us/step - loss: 17.9391 - mae: 2.5900 - val_loss: 46.0161 - val_mae: 3.8874\n",
      "Epoch 291/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.0413 - mae: 2.5690 - val_loss: 44.8625 - val_mae: 3.9807\n",
      "Epoch 292/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.1692 - mae: 2.6281 - val_loss: 44.4366 - val_mae: 3.8561\n",
      "Epoch 293/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.2235 - mae: 2.6676 - val_loss: 46.3302 - val_mae: 3.9488\n",
      "Epoch 294/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.2370 - mae: 2.6561 - val_loss: 47.6038 - val_mae: 4.3173\n",
      "Epoch 295/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.5106 - mae: 2.6865 - val_loss: 46.2897 - val_mae: 3.8818\n",
      "Epoch 296/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 18.1409 - mae: 2.6456 - val_loss: 44.3223 - val_mae: 3.8244\n",
      "Epoch 297/500\n",
      "90/90 [==============================] - 0s 889us/step - loss: 18.2121 - mae: 2.6162 - val_loss: 45.2679 - val_mae: 3.8600\n",
      "Epoch 298/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 18.0185 - mae: 2.6014 - val_loss: 44.9645 - val_mae: 3.9144\n",
      "Epoch 299/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 17.9682 - mae: 2.5940 - val_loss: 45.9185 - val_mae: 3.8786\n",
      "Epoch 300/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 17.7877 - mae: 2.5706 - val_loss: 44.8615 - val_mae: 3.8983\n",
      "Epoch 301/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 18.0773 - mae: 2.5971 - val_loss: 45.1927 - val_mae: 3.8511\n",
      "Epoch 302/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 18.8247 - mae: 2.7599 - val_loss: 46.3437 - val_mae: 4.0013\n",
      "Epoch 303/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.3642 - mae: 2.6373 - val_loss: 45.4903 - val_mae: 3.9512\n",
      "Epoch 304/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.5216 - mae: 2.6760 - val_loss: 45.9075 - val_mae: 3.8879\n",
      "Epoch 305/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.6448 - mae: 2.7174 - val_loss: 45.0427 - val_mae: 3.9826\n",
      "Epoch 306/500\n",
      "90/90 [==============================] - 0s 907us/step - loss: 18.0888 - mae: 2.6216 - val_loss: 44.7278 - val_mae: 3.8794\n",
      "Epoch 307/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.6059 - mae: 2.6757 - val_loss: 47.5842 - val_mae: 4.0225\n",
      "Epoch 308/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.2118 - mae: 2.6411 - val_loss: 45.7183 - val_mae: 3.8095\n",
      "Epoch 309/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 17.8786 - mae: 2.5911 - val_loss: 45.9419 - val_mae: 3.8360\n",
      "Epoch 310/500\n",
      "90/90 [==============================] - 0s 929us/step - loss: 18.2571 - mae: 2.7030 - val_loss: 48.7731 - val_mae: 4.0952\n",
      "Epoch 311/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.6678 - mae: 2.6710 - val_loss: 45.9538 - val_mae: 3.9076\n",
      "Epoch 312/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.5917 - mae: 2.5626 - val_loss: 45.6529 - val_mae: 3.8652\n",
      "Epoch 313/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.9161 - mae: 2.5988 - val_loss: 45.5157 - val_mae: 3.7794\n",
      "Epoch 314/500\n",
      "90/90 [==============================] - 0s 884us/step - loss: 18.1389 - mae: 2.6192 - val_loss: 44.5549 - val_mae: 3.8657\n",
      "Epoch 315/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 17.8895 - mae: 2.6060 - val_loss: 48.7977 - val_mae: 4.0987\n",
      "Epoch 316/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.6547 - mae: 2.5445 - val_loss: 44.6841 - val_mae: 3.8307\n",
      "Epoch 317/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 17.6796 - mae: 2.5826 - val_loss: 46.6215 - val_mae: 4.0427\n",
      "Epoch 318/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 17.8781 - mae: 2.5969 - val_loss: 44.4313 - val_mae: 3.8016\n",
      "Epoch 319/500\n",
      "90/90 [==============================] - 0s 974us/step - loss: 17.8689 - mae: 2.5998 - val_loss: 48.1220 - val_mae: 4.0385\n",
      "Epoch 320/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.8585 - mae: 2.5720 - val_loss: 45.3863 - val_mae: 3.8772\n",
      "Epoch 321/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.2756 - mae: 2.6561 - val_loss: 45.9042 - val_mae: 3.8475\n",
      "Epoch 322/500\n",
      "90/90 [==============================] - 0s 884us/step - loss: 18.2719 - mae: 2.6449 - val_loss: 48.4665 - val_mae: 4.0041\n",
      "Epoch 323/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.3145 - mae: 2.6241 - val_loss: 45.2955 - val_mae: 3.8509\n",
      "Epoch 324/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.0280 - mae: 2.6065 - val_loss: 44.9808 - val_mae: 3.8293\n",
      "Epoch 325/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.9690 - mae: 2.6419 - val_loss: 45.4300 - val_mae: 3.8548\n",
      "Epoch 326/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.8637 - mae: 2.5765 - val_loss: 44.4013 - val_mae: 3.8658\n",
      "Epoch 327/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.6499 - mae: 2.5525 - val_loss: 45.7065 - val_mae: 3.9031\n",
      "Epoch 328/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 19.3922 - mae: 2.7068 - val_loss: 41.2145 - val_mae: 3.8746\n",
      "Epoch 329/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 19.2060 - mae: 2.6928 - val_loss: 44.0521 - val_mae: 3.9278\n",
      "Epoch 330/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.2656 - mae: 2.6436 - val_loss: 47.3892 - val_mae: 4.0650\n",
      "Epoch 331/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.7534 - mae: 2.7063 - val_loss: 45.5360 - val_mae: 3.8699\n",
      "Epoch 332/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.8787 - mae: 2.5763 - val_loss: 46.8636 - val_mae: 3.9062\n",
      "Epoch 333/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.8005 - mae: 2.5880 - val_loss: 44.7282 - val_mae: 3.7786\n",
      "Epoch 334/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 17.8002 - mae: 2.5575 - val_loss: 45.0561 - val_mae: 3.8543\n",
      "Epoch 335/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.9454 - mae: 2.6118 - val_loss: 44.1490 - val_mae: 3.8811\n",
      "Epoch 336/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.8602 - mae: 2.5886 - val_loss: 44.7246 - val_mae: 3.9480\n",
      "Epoch 337/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.2874 - mae: 2.6573 - val_loss: 45.5863 - val_mae: 3.8055\n",
      "Epoch 338/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 17.9279 - mae: 2.5770 - val_loss: 45.7529 - val_mae: 3.9421\n",
      "Epoch 339/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.9360 - mae: 2.6226 - val_loss: 45.4838 - val_mae: 3.8371\n",
      "Epoch 340/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.7971 - mae: 2.5956 - val_loss: 46.8895 - val_mae: 3.8132\n",
      "Epoch 341/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 17.8159 - mae: 2.5755 - val_loss: 46.5874 - val_mae: 3.9725\n",
      "Epoch 342/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 18.0937 - mae: 2.6124 - val_loss: 46.0075 - val_mae: 3.9264\n",
      "Epoch 343/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.6078 - mae: 2.5722 - val_loss: 45.0945 - val_mae: 3.9308\n",
      "Epoch 344/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.0178 - mae: 2.5857 - val_loss: 46.2904 - val_mae: 3.8495\n",
      "Epoch 345/500\n",
      "90/90 [==============================] - 0s 851us/step - loss: 17.7558 - mae: 2.5495 - val_loss: 45.9985 - val_mae: 3.8861\n",
      "Epoch 346/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.9939 - mae: 2.5809 - val_loss: 45.5458 - val_mae: 3.8737\n",
      "Epoch 347/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 17.8664 - mae: 2.5848 - val_loss: 48.0587 - val_mae: 4.0059\n",
      "Epoch 348/500\n",
      "90/90 [==============================] - 0s 963us/step - loss: 18.7234 - mae: 2.6699 - val_loss: 46.4367 - val_mae: 3.8953\n",
      "Epoch 349/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 17.6857 - mae: 2.5721 - val_loss: 46.9490 - val_mae: 4.0344\n",
      "Epoch 350/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.3196 - mae: 2.6490 - val_loss: 44.4750 - val_mae: 3.8949\n",
      "Epoch 351/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.7837 - mae: 2.6050 - val_loss: 45.9512 - val_mae: 3.8537\n",
      "Epoch 352/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.6521 - mae: 2.5448 - val_loss: 45.3511 - val_mae: 3.8647\n",
      "Epoch 353/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 17.9731 - mae: 2.6002 - val_loss: 47.9388 - val_mae: 4.0495\n",
      "Epoch 354/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 18.3593 - mae: 2.6022 - val_loss: 47.3800 - val_mae: 4.0072\n",
      "Epoch 355/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 18.6148 - mae: 2.7249 - val_loss: 46.0107 - val_mae: 3.8407\n",
      "Epoch 356/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.4701 - mae: 2.6680 - val_loss: 45.4106 - val_mae: 3.8336\n",
      "Epoch 357/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 18.2323 - mae: 2.6399 - val_loss: 44.7735 - val_mae: 3.9099\n",
      "Epoch 358/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.8259 - mae: 2.5931 - val_loss: 45.6305 - val_mae: 3.8414\n",
      "Epoch 359/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.7331 - mae: 2.5547 - val_loss: 45.5193 - val_mae: 3.8655\n",
      "Epoch 360/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 17.9415 - mae: 2.5902 - val_loss: 45.4335 - val_mae: 3.9108\n",
      "Epoch 361/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.9404 - mae: 2.5870 - val_loss: 45.4938 - val_mae: 3.9161\n",
      "Epoch 362/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.6153 - mae: 2.5746 - val_loss: 46.9833 - val_mae: 3.9501\n",
      "Epoch 363/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.6785 - mae: 2.5642 - val_loss: 44.7710 - val_mae: 3.8305\n",
      "Epoch 364/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.6467 - mae: 2.5510 - val_loss: 45.9618 - val_mae: 3.8590\n",
      "Epoch 365/500\n",
      "90/90 [==============================] - 0s 851us/step - loss: 17.5818 - mae: 2.5508 - val_loss: 46.5877 - val_mae: 3.8375\n",
      "Epoch 366/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.7275 - mae: 2.6558 - val_loss: 47.9975 - val_mae: 3.9686\n",
      "Epoch 367/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 18.4616 - mae: 2.6176 - val_loss: 46.5916 - val_mae: 3.9535\n",
      "Epoch 368/500\n",
      "90/90 [==============================] - 0s 929us/step - loss: 17.8958 - mae: 2.6161 - val_loss: 45.8731 - val_mae: 3.8878\n",
      "Epoch 369/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.8160 - mae: 2.6009 - val_loss: 46.0284 - val_mae: 3.8135\n",
      "Epoch 370/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 17.8997 - mae: 2.5728 - val_loss: 46.5391 - val_mae: 3.9011\n",
      "Epoch 371/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 18.0949 - mae: 2.6260 - val_loss: 46.6646 - val_mae: 3.8759\n",
      "Epoch 372/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 17.8552 - mae: 2.5764 - val_loss: 44.7477 - val_mae: 3.8457\n",
      "Epoch 373/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 17.3687 - mae: 2.5635 - val_loss: 46.2901 - val_mae: 3.8999\n",
      "Epoch 374/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 17.6511 - mae: 2.5695 - val_loss: 47.7538 - val_mae: 3.9498\n",
      "Epoch 375/500\n",
      "90/90 [==============================] - 0s 985us/step - loss: 17.7090 - mae: 2.5863 - val_loss: 45.4822 - val_mae: 3.9002\n",
      "Epoch 376/500\n",
      "90/90 [==============================] - 0s 851us/step - loss: 17.6681 - mae: 2.5600 - val_loss: 46.5460 - val_mae: 3.8808\n",
      "Epoch 377/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 17.7786 - mae: 2.5691 - val_loss: 45.7974 - val_mae: 3.8989\n",
      "Epoch 378/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 17.7644 - mae: 2.5657 - val_loss: 49.2565 - val_mae: 4.1733\n",
      "Epoch 379/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.8164 - mae: 2.5855 - val_loss: 44.5916 - val_mae: 3.8214\n",
      "Epoch 380/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.8445 - mae: 2.5706 - val_loss: 48.2113 - val_mae: 4.0161\n",
      "Epoch 381/500\n",
      "90/90 [==============================] - 0s 851us/step - loss: 17.5875 - mae: 2.5552 - val_loss: 45.7981 - val_mae: 3.8799\n",
      "Epoch 382/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 17.5519 - mae: 2.5650 - val_loss: 45.7036 - val_mae: 3.8422\n",
      "Epoch 383/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.5143 - mae: 2.5454 - val_loss: 44.8434 - val_mae: 3.7873\n",
      "Epoch 384/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.6635 - mae: 2.5585 - val_loss: 44.7722 - val_mae: 3.9009\n",
      "Epoch 385/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.9854 - mae: 2.5857 - val_loss: 46.1159 - val_mae: 4.0334\n",
      "Epoch 386/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.5690 - mae: 2.5781 - val_loss: 44.8953 - val_mae: 3.8397\n",
      "Epoch 387/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.5657 - mae: 2.5746 - val_loss: 46.4359 - val_mae: 3.8783\n",
      "Epoch 388/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 17.7351 - mae: 2.5964 - val_loss: 45.4413 - val_mae: 4.0238\n",
      "Epoch 389/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.4819 - mae: 2.6692 - val_loss: 43.9674 - val_mae: 3.8258\n",
      "Epoch 390/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 18.0848 - mae: 2.6329 - val_loss: 44.8891 - val_mae: 3.8616\n",
      "Epoch 391/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.9378 - mae: 2.5931 - val_loss: 47.3377 - val_mae: 3.9630\n",
      "Epoch 392/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.4232 - mae: 2.5337 - val_loss: 45.5169 - val_mae: 3.8888\n",
      "Epoch 393/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 17.7359 - mae: 2.5685 - val_loss: 45.9662 - val_mae: 3.8774\n",
      "Epoch 394/500\n",
      "90/90 [==============================] - 0s 918us/step - loss: 17.5227 - mae: 2.5580 - val_loss: 45.1256 - val_mae: 3.9057\n",
      "Epoch 395/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 17.4416 - mae: 2.5516 - val_loss: 47.2919 - val_mae: 3.9472\n",
      "Epoch 396/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.6569 - mae: 2.5812 - val_loss: 47.3067 - val_mae: 3.9309\n",
      "Epoch 397/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.4551 - mae: 2.5389 - val_loss: 44.0689 - val_mae: 3.8889\n",
      "Epoch 398/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 17.8748 - mae: 2.5614 - val_loss: 45.6464 - val_mae: 3.8529\n",
      "Epoch 399/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 17.8172 - mae: 2.5501 - val_loss: 44.2714 - val_mae: 3.8254\n",
      "Epoch 400/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 17.9084 - mae: 2.6215 - val_loss: 45.6158 - val_mae: 3.8860\n",
      "Epoch 401/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 17.8677 - mae: 2.5746 - val_loss: 44.4479 - val_mae: 3.8604\n",
      "Epoch 402/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.7107 - mae: 2.5539 - val_loss: 46.9736 - val_mae: 3.9821\n",
      "Epoch 403/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 17.6402 - mae: 2.5699 - val_loss: 45.2024 - val_mae: 3.9206\n",
      "Epoch 404/500\n",
      "90/90 [==============================] - 0s 952us/step - loss: 17.7170 - mae: 2.5590 - val_loss: 47.0169 - val_mae: 3.9850\n",
      "Epoch 405/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 17.9664 - mae: 2.6080 - val_loss: 46.2872 - val_mae: 3.8462\n",
      "Epoch 406/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 17.8191 - mae: 2.5692 - val_loss: 46.0661 - val_mae: 3.8073\n",
      "Epoch 407/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.7017 - mae: 2.5682 - val_loss: 45.8582 - val_mae: 3.8461\n",
      "Epoch 408/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 17.5751 - mae: 2.5390 - val_loss: 46.5109 - val_mae: 3.9018\n",
      "Epoch 409/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 17.6076 - mae: 2.5626 - val_loss: 45.0686 - val_mae: 3.8250\n",
      "Epoch 410/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.6199 - mae: 2.5596 - val_loss: 45.6731 - val_mae: 3.8805\n",
      "Epoch 411/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.6663 - mae: 2.5886 - val_loss: 45.6457 - val_mae: 3.8706\n",
      "Epoch 412/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 17.7903 - mae: 2.5912 - val_loss: 46.0838 - val_mae: 3.8952\n",
      "Epoch 413/500\n",
      "90/90 [==============================] - 0s 907us/step - loss: 17.6453 - mae: 2.5720 - val_loss: 46.1591 - val_mae: 3.8633\n",
      "Epoch 414/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.4105 - mae: 2.5603 - val_loss: 45.2445 - val_mae: 3.8567\n",
      "Epoch 415/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.5830 - mae: 2.5511 - val_loss: 45.8703 - val_mae: 3.9585\n",
      "Epoch 416/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 17.5692 - mae: 2.5597 - val_loss: 45.7379 - val_mae: 3.9225\n",
      "Epoch 417/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.5636 - mae: 2.5379 - val_loss: 46.8420 - val_mae: 3.9420\n",
      "Epoch 418/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 17.6691 - mae: 2.5690 - val_loss: 45.8224 - val_mae: 3.9448\n",
      "Epoch 419/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.1812 - mae: 2.6462 - val_loss: 47.4380 - val_mae: 4.1364\n",
      "Epoch 420/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 17.7897 - mae: 2.5811 - val_loss: 46.5450 - val_mae: 3.8510\n",
      "Epoch 421/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.5983 - mae: 2.5619 - val_loss: 47.0508 - val_mae: 4.0587\n",
      "Epoch 422/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.7805 - mae: 2.6043 - val_loss: 45.6297 - val_mae: 3.9115\n",
      "Epoch 423/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.9684 - mae: 2.6414 - val_loss: 47.7630 - val_mae: 3.9744\n",
      "Epoch 424/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.0075 - mae: 2.5981 - val_loss: 44.8127 - val_mae: 3.8372\n",
      "Epoch 425/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.6569 - mae: 2.5522 - val_loss: 45.4857 - val_mae: 3.9286\n",
      "Epoch 426/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.4754 - mae: 2.5384 - val_loss: 45.6790 - val_mae: 3.8572\n",
      "Epoch 427/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.2788 - mae: 2.6619 - val_loss: 46.9282 - val_mae: 4.1611\n",
      "Epoch 428/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 17.8251 - mae: 2.5794 - val_loss: 45.0839 - val_mae: 3.8495\n",
      "Epoch 429/500\n",
      "90/90 [==============================] - 0s 963us/step - loss: 17.4792 - mae: 2.5362 - val_loss: 45.4895 - val_mae: 3.9315\n",
      "Epoch 430/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 17.4473 - mae: 2.5372 - val_loss: 45.6491 - val_mae: 3.8684\n",
      "Epoch 431/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.3032 - mae: 2.5137 - val_loss: 46.4106 - val_mae: 3.8876\n",
      "Epoch 432/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.4293 - mae: 2.5306 - val_loss: 46.0158 - val_mae: 3.8885\n",
      "Epoch 433/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.7324 - mae: 2.5681 - val_loss: 48.2977 - val_mae: 4.0974\n",
      "Epoch 434/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.0092 - mae: 2.5848 - val_loss: 45.0834 - val_mae: 3.8473\n",
      "Epoch 435/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.5194 - mae: 2.5539 - val_loss: 45.8896 - val_mae: 3.8910\n",
      "Epoch 436/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.3005 - mae: 2.5294 - val_loss: 47.4795 - val_mae: 3.8722\n",
      "Epoch 437/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.7449 - mae: 2.5918 - val_loss: 46.2424 - val_mae: 4.0465\n",
      "Epoch 438/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 17.6789 - mae: 2.6089 - val_loss: 45.1076 - val_mae: 3.9106\n",
      "Epoch 439/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.5533 - mae: 2.5747 - val_loss: 45.8945 - val_mae: 3.8447\n",
      "Epoch 440/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 17.3177 - mae: 2.5271 - val_loss: 47.5336 - val_mae: 3.9768\n",
      "Epoch 441/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.6140 - mae: 2.5736 - val_loss: 45.8771 - val_mae: 4.0128\n",
      "Epoch 442/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.7385 - mae: 2.5622 - val_loss: 46.9685 - val_mae: 3.9301\n",
      "Epoch 443/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 17.7303 - mae: 2.5622 - val_loss: 45.6694 - val_mae: 3.8632\n",
      "Epoch 444/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.4586 - mae: 2.5315 - val_loss: 45.9701 - val_mae: 3.9410\n",
      "Epoch 445/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.5185 - mae: 2.5517 - val_loss: 46.9825 - val_mae: 3.8941\n",
      "Epoch 446/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.4476 - mae: 2.5288 - val_loss: 45.9807 - val_mae: 3.9262\n",
      "Epoch 447/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.5552 - mae: 2.5456 - val_loss: 45.8703 - val_mae: 3.8997\n",
      "Epoch 448/500\n",
      "90/90 [==============================] - 0s 862us/step - loss: 17.5812 - mae: 2.5679 - val_loss: 46.2473 - val_mae: 3.9411\n",
      "Epoch 449/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.5478 - mae: 2.5573 - val_loss: 46.5556 - val_mae: 3.9466\n",
      "Epoch 450/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.5285 - mae: 2.5512 - val_loss: 44.6294 - val_mae: 3.7985\n",
      "Epoch 451/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.5651 - mae: 2.5637 - val_loss: 46.5325 - val_mae: 3.8845\n",
      "Epoch 452/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.7315 - mae: 2.5586 - val_loss: 43.9882 - val_mae: 3.8709\n",
      "Epoch 453/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.9638 - mae: 2.5861 - val_loss: 45.6245 - val_mae: 3.8695\n",
      "Epoch 454/500\n",
      "90/90 [==============================] - 0s 963us/step - loss: 17.9894 - mae: 2.5850 - val_loss: 45.6235 - val_mae: 3.9707\n",
      "Epoch 455/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.6025 - mae: 2.5480 - val_loss: 49.5641 - val_mae: 4.2569\n",
      "Epoch 456/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.8975 - mae: 2.6163 - val_loss: 45.4245 - val_mae: 3.8383\n",
      "Epoch 457/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.7972 - mae: 2.5691 - val_loss: 45.1060 - val_mae: 3.9206\n",
      "Epoch 458/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 17.7631 - mae: 2.5524 - val_loss: 45.9171 - val_mae: 3.8945\n",
      "Epoch 459/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.3174 - mae: 2.5316 - val_loss: 46.2495 - val_mae: 3.9105\n",
      "Epoch 460/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.6020 - mae: 2.5333 - val_loss: 46.5362 - val_mae: 3.9555\n",
      "Epoch 461/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.4267 - mae: 2.5252 - val_loss: 45.1489 - val_mae: 3.8758\n",
      "Epoch 462/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.4918 - mae: 2.5453 - val_loss: 46.8112 - val_mae: 3.8896\n",
      "Epoch 463/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.8501 - mae: 2.5995 - val_loss: 45.8558 - val_mae: 3.9096\n",
      "Epoch 464/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.3238 - mae: 2.5096 - val_loss: 45.7972 - val_mae: 3.9929\n",
      "Epoch 465/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.4379 - mae: 2.5516 - val_loss: 46.4499 - val_mae: 3.8631\n",
      "Epoch 466/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.6168 - mae: 2.5871 - val_loss: 45.8417 - val_mae: 3.8184\n",
      "Epoch 467/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.5241 - mae: 2.5553 - val_loss: 44.2942 - val_mae: 3.9134\n",
      "Epoch 468/500\n",
      "90/90 [==============================] - 0s 929us/step - loss: 17.7725 - mae: 2.5660 - val_loss: 46.9352 - val_mae: 3.9991\n",
      "Epoch 469/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.8059 - mae: 2.5651 - val_loss: 46.3417 - val_mae: 3.9646\n",
      "Epoch 470/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.6089 - mae: 2.5717 - val_loss: 45.7192 - val_mae: 3.8277\n",
      "Epoch 471/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.5015 - mae: 2.5195 - val_loss: 44.9435 - val_mae: 3.9024\n",
      "Epoch 472/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.3437 - mae: 2.5385 - val_loss: 45.3068 - val_mae: 3.8814\n",
      "Epoch 473/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 18.3395 - mae: 2.6345 - val_loss: 46.0534 - val_mae: 3.8498\n",
      "Epoch 474/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.5744 - mae: 2.5508 - val_loss: 45.6468 - val_mae: 3.8884\n",
      "Epoch 475/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.5092 - mae: 2.5565 - val_loss: 47.1853 - val_mae: 3.9540\n",
      "Epoch 476/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 17.6117 - mae: 2.5854 - val_loss: 45.6907 - val_mae: 3.8855\n",
      "Epoch 477/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.4578 - mae: 2.5292 - val_loss: 46.9274 - val_mae: 3.9414\n",
      "Epoch 478/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.7377 - mae: 2.5244 - val_loss: 46.0529 - val_mae: 3.8410\n",
      "Epoch 479/500\n",
      "90/90 [==============================] - 0s 952us/step - loss: 17.3254 - mae: 2.5283 - val_loss: 46.1237 - val_mae: 3.9838\n",
      "Epoch 480/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.3472 - mae: 2.5528 - val_loss: 45.7267 - val_mae: 3.8709\n",
      "Epoch 481/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.3383 - mae: 2.5340 - val_loss: 47.1513 - val_mae: 3.8796\n",
      "Epoch 482/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.4934 - mae: 2.5311 - val_loss: 46.5156 - val_mae: 3.8737\n",
      "Epoch 483/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.6984 - mae: 2.6091 - val_loss: 45.6994 - val_mae: 3.8004\n",
      "Epoch 484/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 17.3860 - mae: 2.5470 - val_loss: 46.3157 - val_mae: 3.8456\n",
      "Epoch 485/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 18.0880 - mae: 2.6417 - val_loss: 46.3481 - val_mae: 3.8843\n",
      "Epoch 486/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 17.4603 - mae: 2.5536 - val_loss: 47.0198 - val_mae: 3.9226\n",
      "Epoch 487/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.3432 - mae: 2.5009 - val_loss: 45.1192 - val_mae: 3.8262\n",
      "Epoch 488/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.6839 - mae: 2.5693 - val_loss: 45.4444 - val_mae: 3.8766\n",
      "Epoch 489/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 17.5815 - mae: 2.5591 - val_loss: 46.2334 - val_mae: 3.8990\n",
      "Epoch 490/500\n",
      "90/90 [==============================] - 0s 896us/step - loss: 17.5395 - mae: 2.5297 - val_loss: 45.1514 - val_mae: 3.9258\n",
      "Epoch 491/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.5045 - mae: 2.5222 - val_loss: 46.8407 - val_mae: 3.9841\n",
      "Epoch 492/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.4235 - mae: 2.5136 - val_loss: 45.6383 - val_mae: 3.8834\n",
      "Epoch 493/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.2518 - mae: 2.5206 - val_loss: 45.2320 - val_mae: 3.8613\n",
      "Epoch 494/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.2240 - mae: 2.5072 - val_loss: 46.6760 - val_mae: 3.9027\n",
      "Epoch 495/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.4288 - mae: 2.5296 - val_loss: 45.8201 - val_mae: 3.8820\n",
      "Epoch 496/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.4554 - mae: 2.5457 - val_loss: 46.1882 - val_mae: 3.9246\n",
      "Epoch 497/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.3503 - mae: 2.5081 - val_loss: 46.2477 - val_mae: 3.8563\n",
      "Epoch 498/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.4788 - mae: 2.5448 - val_loss: 45.6149 - val_mae: 3.8279\n",
      "Epoch 499/500\n",
      "90/90 [==============================] - 0s 873us/step - loss: 17.3355 - mae: 2.5168 - val_loss: 45.8397 - val_mae: 3.9238\n",
      "Epoch 500/500\n",
      "90/90 [==============================] - 0s 885us/step - loss: 17.5383 - mae: 2.5390 - val_loss: 49.4248 - val_mae: 4.0392\n"
     ]
    }
   ],
   "source": [
    "# compile the model                                                              \n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae',])                 \n",
    "# This callback will stop the training when there is no improvement in           \n",
    "# the validation loss for epochs of 'patience'.\n",
    "                             \n",
    "# fit the model                                                                                                                                                \n",
    "history = model.fit(train_scaled, y_train,                                            \n",
    "                    validation_data=(test_scaled, y_test),                            \n",
    "                    epochs=epochs, batch_size=batch_size,\n",
    "                    verbose=1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
